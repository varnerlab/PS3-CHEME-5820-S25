{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebe7831",
   "metadata": {},
   "source": [
    "# PS3: Let's Classify RNA data using K-Nearest Neighbors (KNN)\n",
    "In this problem set, we will implement the K-Nearest Neighbors (KNN) algorithm to classify RNA data. \n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> By the end of this problem set, you should be able to:\n",
    "> * __Prepare RNA data for distance-based classification:__ Parse the LIBSVM training and test files and package each example as a feature-label pair. Apply z-score scaling to each feature so no single feature range dominates distance calculations.\n",
    "> * __Build and run a kernelized KNN model:__ Define an RBF kernel and verify the sampled kernel matrix is positive semidefinite. Construct a weighted KNN classifier from reference samples and predict labels on a configurable test subset.\n",
    "> * __Evaluate model behavior with task-relevant metrics:__ Compute confusion-matrix counts together with accuracy, precision, recall, balanced accuracy, and F1 for RNA label predictions. Interpret a sweep over `K` and `gamma` to understand tradeoffs between overall correctness and positive-class recovery.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabe961",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> __Environment Setup with Include.jl__\n",
    ">\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/).\n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9ee499",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726cd62b",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1261c",
   "metadata": {},
   "source": [
    "### Data\n",
    "Let's load [a dataset from the `LIBSVM` data archive](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) that describes the detection of non-coding RNA sequences that was initially published by:\n",
    "* [Andrew V Uzilov, Joshua M Keegan, and David H Mathews. Detection of non-coding RNAs on the basis of predicted secondary structure formation free energy change. BMC Bioinformatics, 7(173), 2006.](https://pubmed.ncbi.nlm.nih.gov/16566836/)\n",
    "\n",
    "\n",
    "Non-coding RNAs (ncRNAs) have many roles in cells. However, detecting novel ncRNAs in biochemical screens is challenging. Accurate computational methods for detecting ncRNAs in sequenced genomes are important to understanding the roles ncRNAs play in cells. \n",
    "\n",
    "> __What's in the dataset?__\n",
    "> \n",
    "> In this dataset, there are `59535` training instances in the `training` data; each instance has `8` continuous features and a binary label $y\\in\\left\\{-1,1\\right\\}$, where a label of `1` indicates that the RNA sequence is non-coding and a label of `-1` indicates that the RNA sequence is coding. \n",
    "> \n",
    "> The features are continuous values for each RNA sequence pair consisting of Dynalign-predicted total folding free energy (`ΔG_total`), the shorter-sequence length, and the A/U/C nucleotide frequencies for each sequence.\n",
    "> \n",
    "> The `test` dataset has `271617` instances (with the same `8` continuous features and a binary label).\n",
    "\n",
    "We begin by loading the `training` and `test` datasets. The [`LIBSVM` library authors](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) have developed these subsets. Let's start by setting some constants and then loading the data. Please look at the comment next to the constant for a definition of what it is, units, permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0241898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 8; # there are eight continuous features\n",
    "number_of_tests_to_run = 1000; # how many test examples to run through the KNN algorithm?\n",
    "number_of_reference_samples = 10001; # how many training examples to use as reference samples for the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f750f3",
   "metadata": {},
   "source": [
    "In the code block below we preprocess the `training` dataset. We have [z-score centered](https://en.wikipedia.org/wiki/Standard_score) the training data and combined it into an array where each row is a training instance, while the first `1:number_of_features` columns hold the features. The last column has the label. \n",
    "\n",
    "We store the training data in the `training::Array{NamedTuple,1}` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033a7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = let\n",
    "\n",
    "    # load the training data -\n",
    "    data = parser(joinpath(_PATH_TO_DATA, \"cod-rna-training.data\"));\n",
    "    number_of_rows = size(data,1);\n",
    "    data_perm = randperm(number_of_rows);\n",
    "    training_data = Array{NamedTuple,1}(undef, number_of_rows);\n",
    "    X = data[data_perm,:];\n",
    "    \n",
    "    # z-score center the data -\n",
    "    μ = mean(X[:,1:number_of_features],dims=1);\n",
    "    σ = std(X[:,1:number_of_features],dims=1);\n",
    "    X̂ = zeros(number_of_rows,number_of_features+1);\n",
    "    for i ∈ 1:number_of_rows\n",
    "        for j ∈ 1:number_of_features\n",
    "            X̂[i,j] = (X[i,j] - μ[j])/(σ[j]);\n",
    "        end\n",
    "        X̂[i,end] = X[i,end]; # get the label\n",
    "    end\n",
    "\n",
    "    # package the data into an array of named tuples -\n",
    "    for i ∈ 1:number_of_rows\n",
    "        features = X̂[i,1:number_of_features];\n",
    "        label = X̂[i,end];\n",
    "        training_data[i] = (x = features, y = (label |> Int ));\n",
    "    end\n",
    "\n",
    "    training_data; # return scaled - balanced data\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd5436",
   "metadata": {},
   "source": [
    "Next, we preprocess the `test` dataset.  We [z-score center](https://en.wikipedia.org/wiki/Standard_score) the test data and combined it into an array where each row is a test instance, while the first `1:number_of_features` columns hold the features. The last column has the label.\n",
    "\n",
    "We store the test data in the `test::Array{NamedTuple,1}` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b067bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = let\n",
    "\n",
    "    # load the training data -\n",
    "    data = parser(joinpath(_PATH_TO_DATA, \"cod-rna-testing.data\"));\n",
    "    number_of_rows = size(data,1);\n",
    "    data_perm = randperm(number_of_rows);\n",
    "    test_data = Array{NamedTuple,1}(undef, number_of_rows);\n",
    "    X = data[data_perm,:];\n",
    "    \n",
    "    # z-score center the data -\n",
    "    μ = mean(X[:,1:number_of_features],dims=1);\n",
    "    σ = std(X[:,1:number_of_features],dims=1);\n",
    "    X̂ = zeros(number_of_rows,number_of_features+1);\n",
    "    for i ∈ 1:number_of_rows\n",
    "        for j ∈ 1:number_of_features\n",
    "            X̂[i,j] = (X[i,j] - μ[j])/(σ[j]);\n",
    "        end\n",
    "        X̂[i,end] = X[i,end]; # get the label\n",
    "    end\n",
    "\n",
    "    # package the data into an array of named tuples -\n",
    "    for i ∈ 1:number_of_rows\n",
    "        features = X̂[i,1:number_of_features];\n",
    "        label = X̂[i,end];\n",
    "        test_data[i] = (x=features, y = (label |> Int ));\n",
    "    end\n",
    "\n",
    "    test_data; # return scaled - balanced data\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f38bb0",
   "metadata": {},
   "source": [
    "> __Are the labels in the training dataset balanced?__\n",
    ">\n",
    "> Since we are using a KNN classifier, it is important to check label balance in the reference dataset. If labels are imbalanced, neighbor voting can bias predictions toward the majority class and reduce minority-class performance.\n",
    "\n",
    "Let's compute the positive and negative label fractions in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224314eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of positive labels in the training dataset: 0.3333333333333333\n",
      "Fraction of negative labels in the training dataset: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "let \n",
    "\n",
    "    # initialize -\n",
    "    D = training; # specify what data set we are working with\n",
    "    number_of_training_examples = length(D); # how many examples are in the training dataset?\n",
    "\n",
    "    # Fancy! Let's use a list comprehension to count the number of positive and negative labels in the training dataset. The `sum()` function will sum up the number of times the condition is true for each example in the training dataset.\n",
    "    count_positive_labels = sum(D[i].y == 1 for i ∈ 1:number_of_training_examples); # how many positive labels are in the training dataset?\n",
    "    count_negative_labels = sum(D[i].y == -1 for i ∈ 1:number_of_training_examples); # how many negative labels are in the training dataset?\n",
    "    \n",
    "    # print the results (fraction of positive and negative labels in the training dataset)\n",
    "    println(\"Fraction of positive labels in the training dataset: \", count_positive_labels / number_of_training_examples);\n",
    "    println(\"Fraction of negative labels in the training dataset: \", count_negative_labels / number_of_training_examples);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ae6de",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa5a2",
   "metadata": {},
   "source": [
    "## Task 2: Let's look at our KNN Classifier\n",
    "In this task, we will build and evaluate the KNN classifier that we will be using to make predictions on the test dataset. We'll build [a `MyWeightedKernelizedKNNClassificationModel` model](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.MyWeightedKernelizedKNNClassificationModel) using the `training` dataset as the reference data.\n",
    "\n",
    "Let's build a kernel function $k:\\mathbb{R}^{m}\\times\\mathbb{R}^{m}\\to\\mathbb{R}$ to measure similarity. For now, let's make up our own kernel function, and save this function in the `k(x,y)::Function` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa8a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k(x,y,γ) = exp(-γ * norm(x-y,2)^2) # RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56aaf7",
   "metadata": {},
   "source": [
    "#### Check: Are we using a valid Kernel function?\n",
    "Let's check to see if the distance (similarity) metric we built is a valid kernel function.\n",
    "\n",
    "> __Condition:__\n",
    ">\n",
    "> A function $k:\\mathbb{R}^{m}\\times\\mathbb{R}^{m}\\to\\mathbb{R}$ is a _valid kernel function_ if and only if the kernel matrix $\\mathbf{K}\\in\\mathbb{R}^{n\\times{n}}$ is positive (semi)definite for all possible choices of the data vectors $\\mathbf{v}_i$, where $K_{ij} = k(\\mathbf{v}_i, \\mathbf{v}_j)$. If $\\mathbf{K}$ is positive (semi)definite, then for any real-valued vector $\\mathbf{x} \\in \\mathbb{R}^n$, the kernel matrix $\\mathbf{K}$ must satisfy $\\mathbf{x}^{\\top}\\mathbf{K}\\mathbf{x} \\geq 0$. \n",
    "\n",
    "Let's compute the kernel matrix `KM::Array{Float64,2}` for a data matrix `X::Array{Float64,2}` using the distance/kernel function `k(x,y)::Function` we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4dfa6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = let\n",
    "\n",
    "    D = training; # specify what data set we are working with\n",
    "    number_of_training_examples = 100; # how many examples are in the training dataset (we will use only small number examples to compute the kernel matrix for computational efficiency)\n",
    "    number_of_features = D[1].x |> length; # number of features in the dataset\n",
    "    γ = 0.5; # kernel parameter\n",
    "\n",
    "    # fill up the feature matrix -\n",
    "    X = zeros(number_of_training_examples, number_of_features); # initialize a matrix to hold the features\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        X[i,:] = D[i].x; # fill the matrix with the features from the training dataset\n",
    "    end\n",
    "\n",
    "    # fill up the kernel matrix -\n",
    "    K = zeros(number_of_training_examples,number_of_training_examples);\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        vᵢ = X[i,:];\n",
    "        for j ∈ 1:number_of_training_examples\n",
    "            vⱼ = X[j,:];\n",
    "            K[i,j] = k(vᵢ,vⱼ,γ) # compute kernel value\n",
    "        end\n",
    "    end\n",
    "    K\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115937a2",
   "metadata": {},
   "source": [
    "Next, let's check to see if the kernel matrix `K::Array{Float64,2}` is positive (semi)definite by checking if all of its eigenvalues are non-negative.\n",
    "\n",
    "> __Check:__\n",
    ">\n",
    "> For this kernel to be valid, the kernel matrix $\\mathbf{K}$ needs to be positive (semi)definite, i.e., all eigenvalues $\\lambda_i \\geq 0$. We compute the eigenvalues using [`eigvals`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigvals) and verify they are all non-negative using [the `@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) in combination with [the `all` function](https://docs.julialang.org/en/v1/base/collections/#Base.all-Tuple%7BAny%7D).\n",
    "\n",
    "Do we blow up? If not, the matrix is PSD for this dataset, which supports using this kernel in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae360c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    λ = eigvals(KM);\n",
    "    @assert all(λ .≥ -1e-10) \"Kernel matrix is not PSD: min eigenvalue = $(minimum(λ))\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1840d7",
   "metadata": {},
   "source": [
    "Ok, so now let's build the KNN classifier model. There are a few design choices to make, such as the number of neighbors to consider (`K`), the kernel function, and any kernel parameters. \n",
    "\n",
    "> __How do we choose the `K` parameter?__\n",
    ">\n",
    "> The choice of `K` controls the bias-variance tradeoff: __bias__ means a systematic error from an oversimplified neighborhood vote, and __variance__ means sensitivity to which training points happen to be in the dataset. \n",
    "> \n",
    "> A small `K` uses local voting (low bias, high variance), while large `K` uses global voting (high bias, low variance). We use $K = mC + 1$, where $m \\geq 0$ is adjustable and $C$ is the number of classes, to explore this tradeoff systematically. \n",
    "\n",
    "Can we see this tradeoff in action by varying `K`? Let's test that on the dataset. Next let's consider our second design choice: the kernel width parameter $\\gamma$ in the RBF kernel.\n",
    "\n",
    "> __Gating parameter $\\gamma$:__\n",
    ">\n",
    "> The parameter $\\gamma>0$ controls how quickly similarity decays with squared distance in $k(\\mathbf{x},\\mathbf{y})=\\exp\\left(-\\gamma\\lVert\\mathbf{x}-\\mathbf{y}\\rVert_2^2\\right)$. Larger $\\gamma$ makes the similarity measure more sensitive, so even small-to-moderate $\\lVert\\mathbf{x}-\\mathbf{y}\\rVert_2^2$ values can drive similarity toward zero and emphasize very local neighbors. Smaller $\\gamma$ makes the kernel less sensitive, so even larger squared-distance differences can still produce moderate similarity and allow more distant neighbors to influence the vote.\n",
    "\n",
    "Let's see how varying $\\gamma$ and the number of neighbors `K` affects the performance of our KNN classifier on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92cee9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "    \n",
    "    # initialize -\n",
    "    D = training; # specify what data set we are working with\n",
    "    number_of_training_examples = number_of_reference_samples; # how many examples are in the training dataset?\n",
    "    number_of_features = D[1].x |> length; # number of features in the dataset\n",
    "    γ = 0.50; # kernel parameter\n",
    "    m = 10; # neighborhood size multiple\n",
    "    C = 2; # number of classes\n",
    "\n",
    "    # fill up the feature matrix -\n",
    "    X = zeros(number_of_training_examples, number_of_features); # initialize a matrix to hold the features\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        X[i,:] = D[i].x; # fill the matrix with the features from the training dataset\n",
    "    end\n",
    "\n",
    "    # fill up the label vector -\n",
    "    y = zeros(number_of_training_examples); # initialize a vector to hold the labels\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        y[i] = D[i].y; # fill the vector with the labels from the training dataset\n",
    "    end\n",
    "\n",
    "    # build a model -\n",
    "    model = build(MyWeightedKernelizedKNNClassificationModel, (\n",
    "        K = (m*C+1), # we look at this many points\n",
    "        features = X,\n",
    "        labels = y,\n",
    "        k = (x,y) -> k(x,y,γ), # RBF kernel similarity metric\n",
    "    ));\n",
    "\n",
    "    model; # return the model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b38ed5",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Now that we have defined a kernel function, and built the model, let's use it to classify our data. We use the KNN classifier from [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). \n",
    "\n",
    "> __What is going on in this code block?__\n",
    ">\n",
    "> In the code block below, we:\n",
    "> * Construct [a `MyWeightedKernelizedKNNClassificationModel` model](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.MyWeightedKernelizedKNNClassificationModel) using [a `build(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/factory/). The `model` instance holds the data for the problem, i.e., how many neighbors to look at `K`, and the kernel function $k$.\n",
    "> * Next, we pass this `model` instance to [the `classify(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.classify) which takes a test feature $\\mathbf{z}$ and the classifier `model` instance and returns the predicted label value $\\hat{y}$ for the test feature vector $\\mathbf{z}$.\n",
    "\n",
    "We return the predicted labels in `ŷ_KNN` and the actual labels in `y_KNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a60a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_KNN,y_KNN = let\n",
    "\n",
    "    # Data -\n",
    "    D = test; # what dataset are we working with?\n",
    "    number_of_test_examples = number_of_tests_to_run; # how many samples are we going to test?\n",
    "    number_of_features = D[1].x |> length; # number of features in the dataset\n",
    "\n",
    "     # fill up the feature matrix -\n",
    "    X = zeros(number_of_test_examples, number_of_features); # initialize a matrix to hold the features\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        X[i,:] = D[i].x; # fill the matrix with the features from the test dataset\n",
    "    end\n",
    "\n",
    "    # fill up the label vector (actual labels) -\n",
    "    y = zeros(number_of_test_examples); # initialize a vector to hold the labels\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        y[i] = D[i].y; # fill the vector with the labels from the test dataset\n",
    "    end\n",
    "\n",
    "    # process each vector in the test set, compare that to training (reference), and compute the predicted label -\n",
    "    ŷ = zeros(number_of_test_examples);  # initialize some storage for the predicted label\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        z = X[i,:]; # get feature vector for test\n",
    "        ŷ[i] = classify(z,model) # classify the test vector using the training data\n",
    "    end\n",
    " \n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3502e44",
   "metadata": {},
   "source": [
    "### Performance\n",
    "We can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the `test` dataset and measure wins (when the label is the same) and losses (label is different). This is easily represented in [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "> __Error analysis using the confusion matrix__\n",
    ">\n",
    "> Total mistakes (or mistake percentage) is only part of the story. We should understand whether we are biased toward false positives or false negatives for RNA labels. \n",
    ">\n",
    "> The confusion matrix for a binary classifier is typically structured as:\n",
    ">\n",
    ">|                     | **Predicted Positive** | **Predicted Negative** |\n",
    ">|---------------------|------------------------|------------------------|\n",
    ">| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    ">| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    ">\n",
    "> From the confusion matrix, we derive five key metrics:\n",
    ">\n",
    "> * __Accuracy__ is the fraction of correct predictions overall: $\\texttt{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$. It tells us the overall success rate but can be misleading if classes are imbalanced; a classifier that predicts \"negative\" for everything might achieve 95% accuracy on an imbalanced dataset.\n",
    "> * __Precision__ answers: \"When we predict positive, how often are we right?\" $\\texttt{precision} = \\frac{TP}{TP + FP}$. In this RNA context, high precision means fewer false positives; if we predict non-coding, that prediction is usually correct. Low precision means many coding sequences are predicted as non-coding.\n",
    "> * __Recall__ (also called sensitivity) answers: \"Of all the true positives, how many did we catch?\" $\\texttt{recall} = \\frac{TP}{TP + FN}$. High recall means we are catching most non-coding sequences. Low recall means many non-coding sequences are predicted as coding.\n",
    "> * __Balanced Accuracy__ averages recall (sensitivity) and specificity: $\\texttt{balanced\\_accuracy} = \\frac{1}{2}\\left(\\frac{TP}{TP + FN} + \\frac{TN}{TN + FP}\\right)$. A high balanced accuracy means the model performs well on both classes, while a low balanced accuracy means the model is weak on at least one class.\n",
    "> * __F1 Score__ is the harmonic mean of precision and recall: $\\texttt{F1} = \\frac{2\\cdot\\texttt{precision}\\cdot\\texttt{recall}}{\\texttt{precision} + \\texttt{recall}}$. A high F1 means we are balancing missed positives and false positives well, while a low F1 means one or both of those error types are large.\n",
    "\n",
    "We compute the __confusion matrix__ to get these counts. We use the [confusion(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion), which takes actual labels and estimated labels, returning the confusion matrix. We save the confusion matrix in the `CM_KNN::Array{Int64,2}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf9c2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       "  52  276\n",
       " 123  549"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_KNN = confusion(y_KNN,ŷ_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cea59",
   "metadata": {},
   "source": [
    "Let's compute the __accuracy__, __precision__, __recall__, __balanced_accuracy__, and __f1__ for our KNN classifier using the confusion matrix. We save these metrics in the `accuracy_KNN::Float64`, `precision_KNN::Float64`, `recall_KNN::Float64`, `balanced_accuracy_KNN::Float64`, and `f1_KNN::Float64` variables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da638bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 60.1%\n",
      "KNN Precision: 29.71%\n",
      "KNN Recall: 15.85%\n",
      "KNN Balanced Accuracy: 48.78%\n",
      "KNN f1: 20.68%\n"
     ]
    }
   ],
   "source": [
    "accuracy_KNN, precision_KNN, recall_KNN, balanced_accuracy_KNN, f1_KNN = let\n",
    "\n",
    "    # compute the confusion matrix -\n",
    "    CM = CM_KNN; # confusion matrix for KNN classifier\n",
    "\n",
    "    # compute accuracy, precision, recall, balanced accuracy, and F1 -\n",
    "    TP = CM[1,1]; # true positives\n",
    "    TN = CM[2,2]; # true negatives\n",
    "    FP = CM[2,1]; # false positives\n",
    "    FN = CM[1,2]; # false negatives\n",
    "\n",
    "    accuracy = (TP + TN) / sum(CM); # overall accuracy\n",
    "    precision = (TP + FP) == 0 ? 0.0 : TP / (TP + FP); # precision\n",
    "    recall = (TP + FN) == 0 ? 0.0 : TP / (TP + FN); # recall\n",
    "    specificity = (TN + FP) == 0 ? 0.0 : TN / (TN + FP); # specificity\n",
    "    balanced_accuracy = 0.5 * (recall + specificity); # balanced accuracy\n",
    "    f1 = (precision + recall) == 0 ? 0.0 : 2 * precision * recall / (precision + recall); # F1 score\n",
    "\n",
    "    # print so the user can see -\n",
    "    println(\"KNN Accuracy: $(round(accuracy*100,digits=2))%\")\n",
    "    println(\"KNN Precision: $(round(precision*100,digits=2))%\")\n",
    "    println(\"KNN Recall: $(round(recall*100,digits=2))%\")\n",
    "    println(\"KNN Balanced Accuracy: $(round(balanced_accuracy*100,digits=2))%\")\n",
    "    println(\"KNN f1: $(round(f1*100,digits=2))%\")\n",
    "\n",
    "    (accuracy, precision, recall, balanced_accuracy, f1); # return the metrics\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e6d63",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c3d6b",
   "metadata": {},
   "source": [
    "## Task 3: Let's do a hyperparameter sweep\n",
    "In this task, we perform a grid search to see how the number of neighbors `K` and kernel gating parameter $\\gamma$ affect classification performance on the test dataset.\n",
    "\n",
    "> __What is going on in this code block?__\n",
    ">\n",
    "> We sweep over several $\\left(K,\\gamma\\right)$ combinations. For each pair, we build a `MyWeightedKernelizedKNNClassificationModel`, classify every test point, compute a confusion matrix, then calculate accuracy, precision, recall, F1, and balanced accuracy. Finally, we sort all results (by F1) so the best precision-recall tradeoff settings appear first.\n",
    "\n",
    "Let's execute the sweep and inspect the top-performing settings. We store the results in the `knn_sweep_results::DataFrame` variable, which contains the `K`, `gamma`, `TP`, `TN`, `FP`, `FN`, `accuracy`, `precision`, `recall`, `f1`, and `balanced_accuracy` values for each combination. We sort this DataFrame by F1 in descending order to see the strongest precision-recall settings at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9644994",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_sweep_results = let\n",
    "\n",
    "    # load data -\n",
    "    Dtr = training;\n",
    "    Dte = test;\n",
    "    ntr = number_of_reference_samples; # how many training examples do we have?\n",
    "    number_of_test_examples = number_of_tests_to_run; # how many samples are we going to test?\n",
    "\n",
    "    # feature/label arrays for training -\n",
    "    Xtr = zeros(ntr, number_of_features);\n",
    "    ytr = zeros(ntr);\n",
    "    for i in 1:ntr\n",
    "        Xtr[i, :] = Dtr[i].x;\n",
    "        ytr[i] = Dtr[i].y;\n",
    "    end\n",
    "\n",
    "    # feature/label arrays for test -\n",
    "    Xte = zeros(number_of_test_examples, number_of_features);\n",
    "    yte = zeros(number_of_test_examples);\n",
    "    for i in 1:number_of_test_examples\n",
    "        Xte[i, :] = Dte[i].x;\n",
    "        yte[i] = Dte[i].y;\n",
    "    end\n",
    "\n",
    "    # hyperparameter grids -\n",
    "    K_grid = [3, 5, 9, 15, 25, 41, 65, 101, 151, 201, 301];\n",
    "    γ_grid = [0.01, 0.05, 0.10, 0.25, 0.50];\n",
    "\n",
    "    # initialize a DataFrame to hold the results of the sweep -\n",
    "    results = DataFrame(\n",
    "        K = Int[],\n",
    "        gamma = Float64[],\n",
    "        TP = Int[],\n",
    "        TN = Int[],\n",
    "        FP = Int[],\n",
    "        FN = Int[],\n",
    "        accuracy = Float64[],\n",
    "        precision = Float64[],\n",
    "        recall = Float64[],\n",
    "        f1 = Float64[],\n",
    "        balanced_accuracy = Float64[]\n",
    "    );\n",
    "\n",
    "    # sweep all (K, gamma) combinations -\n",
    "    for γ ∈ γ_grid\n",
    "        for K_neighbors ∈ K_grid\n",
    "            \n",
    "            # Create a local model for this combination of hyperparameters -\n",
    "            model_local = build(MyWeightedKernelizedKNNClassificationModel, (\n",
    "                K = K_neighbors, # we are looking at this many neighbors\n",
    "                features = Xtr,\n",
    "                labels = ytr,\n",
    "                k = (x, y) -> k(x, y, γ), # RBF kernel with the current gamma\n",
    "            ));\n",
    "\n",
    "            # classify test set -\n",
    "            ŷ = zeros(number_of_test_examples);\n",
    "            for i ∈ 1:number_of_test_examples\n",
    "                ŷ[i] = classify(Xte[i, :], model_local);\n",
    "            end\n",
    "\n",
    "            # confusion-matrix metrics -\n",
    "            CM = confusion(yte, ŷ);\n",
    "            TP = CM[1, 1]; # true positives\n",
    "            TN = CM[2, 2]; # true negatives\n",
    "            FP = CM[2, 1]; # false positives\n",
    "            FN = CM[1, 2]; # false negatives\n",
    "\n",
    "            accuracy = (TP + TN) / sum(CM);\n",
    "            precision = (TP + FP) == 0 ? 0.0 : TP / (TP + FP);\n",
    "            recall = (TP + FN) == 0 ? 0.0 : TP / (TP + FN);\n",
    "\n",
    "            specificity = (TN + FP) == 0 ? 0.0 : TN / (TN + FP)\n",
    "            balanced_accuracy = 0.5 * (recall + specificity)\n",
    "            f1_score = (precision + recall) == 0 ? 0.0 : 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "            # package results into the DataFrame -\n",
    "            push!(results, (\n",
    "                K = K_neighbors, # how many neighbors were used?\n",
    "                gamma = γ, # what was the kernel parameter?\n",
    "                TP = TP, # true positives\n",
    "                TN = TN, # true negatives\n",
    "                FP = FP, # false positives\n",
    "                FN = FN, # false negatives\n",
    "                accuracy = accuracy, # overall accuracy\n",
    "                precision = precision, # precision\n",
    "                recall = recall, # recall\n",
    "                f1 = f1_score, # F1 score\n",
    "                balanced_accuracy = balanced_accuracy, # balanced accuracy\n",
    "            ));\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # sort the results -\n",
    "    df = sort(results, :f1, rev = true); # sort results by F1 (descending order);\n",
    "    df[1:10, :] # return the top 10 results\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf487be9",
   "metadata": {},
   "source": [
    "Next, let's build a table of the results from our hyperparameter search using [the `pretty_table(...)` function exported by the `PrettyTables.jl` package](https://github.com/ronisbr/PrettyTables.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1644efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- --------- ------- ------- ------- ------- ---------- ----------- ---------- ---------- -------------------\n",
      " \u001b[1m     K \u001b[0m \u001b[1m   gamma \u001b[0m \u001b[1m    TP \u001b[0m \u001b[1m    TN \u001b[0m \u001b[1m    FP \u001b[0m \u001b[1m    FN \u001b[0m \u001b[1m accuracy \u001b[0m \u001b[1m precision \u001b[0m \u001b[1m   recall \u001b[0m \u001b[1m       f1 \u001b[0m \u001b[1m balanced_accuracy \u001b[0m\n",
      " \u001b[90m Int64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m   Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m           Float64 \u001b[0m\n",
      " ------- --------- ------- ------- ------- ------- ---------- ----------- ---------- ---------- -------------------\n",
      "      5       0.5     241     192     480      87      0.433    0.334258   0.734756   0.459485            0.510235\n",
      "      5      0.25     235     199     473      93      0.434    0.331921   0.716463   0.453668            0.506297\n",
      "     15       0.5      89     523     149     239      0.612     0.37395   0.271341   0.314488            0.524808\n",
      "      5       0.1      91     486     186     237      0.577     0.32852   0.277439   0.300826            0.500327\n",
      "     15      0.25      71     561     111     257      0.632     0.39011   0.216463   0.278431            0.525642\n",
      "      9       0.5      69     550     122     259      0.619    0.361257   0.210366   0.265896            0.514409\n",
      "     15       0.1      54     598      74     274      0.652    0.421875   0.164634   0.236842            0.527258\n",
      "      9      0.25      38     603      69     290      0.641     0.35514   0.115854   0.174713            0.506588\n",
      "     15      0.05      35     625      47     293       0.66    0.426829   0.106707   0.170732            0.518383\n",
      "     25       0.5      41     553     119     287      0.594     0.25625      0.125   0.168033            0.473958\n",
      " ------- --------- ------- ------- ------- ------- ---------- ----------- ---------- ---------- -------------------\n"
     ]
    }
   ],
   "source": [
    "pretty_table(knn_sweep_results; \n",
    "    backend = :text,\n",
    "    fit_table_in_display_horizontally = false,\n",
    "    table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf99f29",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "We now have set up our KNN classifier for the RNA sequence classification task. Let's explore some of the results and discuss what we see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb361a5",
   "metadata": {},
   "source": [
    "**DQ1: Why is feature scaling essential for KNN on this RNA dataset?** KNN predictions are driven by distances between feature vectors. If one feature (for example, sequence length or total folding energy) spans a much larger numeric range than the nucleotide-frequency features, it can dominate distance calculations and bias neighbor voting.\n",
    "\n",
    "> __Strategy__: Use the preprocessing cells to explain what z-score scaling is doing here and why it matters for Euclidean-distance KNN with an RBF kernel. In 2-3 sentences, describe what could go wrong if we skipped scaling and which features would likely dominate (you can modify the scaling cell to explore your answer).\n",
    "\n",
    "Write your response in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3613ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer DQ1 here after evaluating why scaling matters for KNN distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c50fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ1 = true; # TODO: update to true if answered DQ1 {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ca139",
   "metadata": {},
   "source": [
    "**DQ2: What do accuracy, precision, recall, balanced accuracy, and F1 mean for this RNA classification problem?** This dataset is class-imbalanced, so high accuracy can still occur even when the model misses many true positives. Precision and recall tell us different things about false positives versus false negatives, which can change how we judge model quality.\n",
    "\n",
    "> __Strategy__: Using `CM_KNN`, `accuracy_KNN`, `precision_KNN`, `recall_KNN`, `balanced_accuracy_KNN`, and `f1_KNN`, interpret each metric in the context of predicting RNA class labels. In 2-3 sentences, state whether this model is better at ruling out negatives or finding positives, and which metric you would prioritize for this task.\n",
    "\n",
    "Write your response in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cabddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer DQ2 here after interpreting CM_KNN and the three metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6efb66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ2 = true; # TODO: update to true if answered DQ2 {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e29054",
   "metadata": {},
   "source": [
    "**DQ3: What does the hyperparameter sweep tell us about KNN behavior on this dataset?** The `knn_sweep_results` table shows how changing `K` and `gamma` affects TP, TN, FP, FN, and the derived metrics. Similar accuracies can still correspond to very different precision/recall tradeoffs, which reveals what the model is optimizing in practice.\n",
    "\n",
    "> __Strategy__: Inspect the top-performing and lower-performing rows in `knn_sweep_results`. In 2-3 sentences, describe the trend you see as `K` and `gamma` change, and explain what that suggests about local-vs-global voting and the model's ability to recover positive examples.\n",
    "\n",
    "Write your response in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "188b410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer DQ3 here after analyzing trends in `knn_sweep_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90e0d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ3 = true; # TODO: update to true if answered DQ3 {true | false}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ee274",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d9ace",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This problem set implemented an end-to-end kernelized KNN workflow for classifying RNA sequences as coding or non-coding.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * **Feature scaling controls distance behavior:** Z-score scaling puts all features on comparable numeric ranges before neighbor search. Without scaling, large-range features can dominate distances and distort KNN votes.\n",
    "> * **Confusion-matrix metrics reveal different error modes:** Accuracy summarizes total correctness across both classes. Precision and recall show whether the model is producing too many false non-coding calls or missing true non-coding sequences.\n",
    "> * **Hyperparameter sweeps expose model tradeoffs:** Changing `K` and `gamma` changes how local the voting behavior is and how fast similarity decays. Similar accuracy values can hide large recall differences, so parameter selection should match the task objective.\n",
    "\n",
    "The next step is to choose `K` and `gamma` based on the error type you want to minimize for RNA screening.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fddd90",
   "metadata": {},
   "source": [
    "## Tests\n",
    "In the code block below, we check some values in your notebook and give you feedback on which items are correct or different. `Unhide` the code block below (if you are curious) about how we implemented the tests and what we are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26b23c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:                                   | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "CHEME 5820 problem set 3 test suite             | \u001b[32m 109  \u001b[39m\u001b[36m  109  \u001b[39m\u001b[0m1.0s\n",
      "  Setup, Data, and Prerequisites                | \u001b[32m  15  \u001b[39m\u001b[36m   15  \u001b[39m\u001b[0m0.5s\n",
      "  Task 2: Kernel Function, Model, and Inference | \u001b[32m  14  \u001b[39m\u001b[36m   14  \u001b[39m\u001b[0m0.2s\n",
      "  Task 2: Confusion Matrix and Metrics          | \u001b[32m  14  \u001b[39m\u001b[36m   14  \u001b[39m\u001b[0m0.1s\n",
      "  Task 3: Hyperparameter Sweep                  | \u001b[32m  63  \u001b[39m\u001b[36m   63  \u001b[39m\u001b[0m0.2s\n",
      "  Discussion Questions                          | \u001b[32m   3  \u001b[39m\u001b[36m    3  \u001b[39m\u001b[0m0.0s\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    @testset verbose = true \"CHEME 5820 problem set 3 test suite\" begin\n",
    "\n",
    "        @testset \"Setup, Data, and Prerequisites\" begin\n",
    "            @test number_of_features == 8\n",
    "            @test number_of_tests_to_run > 0\n",
    "            @test number_of_reference_samples > 0\n",
    "            @test isnothing(training) == false\n",
    "            @test isnothing(test) == false\n",
    "            @test length(training) > number_of_reference_samples\n",
    "            @test length(test) > number_of_tests_to_run\n",
    "\n",
    "            ncheck_train = min(1000, length(training))\n",
    "            ncheck_test = min(1000, length(test))\n",
    "            @test all(d -> length(d.x) == number_of_features, training[1:ncheck_train])\n",
    "            @test all(d -> length(d.x) == number_of_features, test[1:ncheck_test])\n",
    "            @test all(d -> d.y in (-1, 1), training[1:ncheck_train])\n",
    "            @test all(d -> d.y in (-1, 1), test[1:ncheck_test])\n",
    "\n",
    "            count_positive_labels_training = sum(training[i].y == 1 for i in eachindex(training))\n",
    "            count_negative_labels_training = sum(training[i].y == -1 for i in eachindex(training))\n",
    "            count_positive_labels_test = sum(test[i].y == 1 for i in eachindex(test))\n",
    "            count_negative_labels_test = sum(test[i].y == -1 for i in eachindex(test))\n",
    "            @test count_positive_labels_training > 0\n",
    "            @test count_negative_labels_training > 0\n",
    "            @test count_positive_labels_test > 0\n",
    "            @test count_negative_labels_test > 0\n",
    "        end\n",
    "\n",
    "        @testset \"Task 2: Kernel Function, Model, and Inference\" begin\n",
    "            @test isnothing(k) == false\n",
    "            @test k(training[1].x, training[2].x, 0.5) isa Real\n",
    "\n",
    "            @test isnothing(KM) == false\n",
    "            km_n1, km_n2 = size(KM)\n",
    "            @test km_n1 == km_n2\n",
    "            @test km_n1 >= 2\n",
    "            @test all(isfinite, KM)\n",
    "            @test isapprox(KM, transpose(KM), atol=1e-8)\n",
    "            @test all(isapprox.(diag(KM), 1.0, atol=1e-8))\n",
    "            λ = eigvals(Symmetric(KM))\n",
    "            @test minimum(λ) >= -1e-8\n",
    "\n",
    "            @test isnothing(model) == false\n",
    "            @test length(ŷ_KNN) == number_of_tests_to_run\n",
    "            @test length(y_KNN) == number_of_tests_to_run\n",
    "            @test all(v -> v in (-1.0, 1.0), ŷ_KNN)\n",
    "            @test all(v -> v in (-1.0, 1.0), y_KNN)\n",
    "        end\n",
    "\n",
    "        @testset \"Task 2: Confusion Matrix and Metrics\" begin\n",
    "            @test isnothing(CM_KNN) == false\n",
    "            @test size(CM_KNN) == (2, 2)\n",
    "            @test all(CM_KNN .>= 0)\n",
    "            @test sum(CM_KNN) == number_of_tests_to_run\n",
    "\n",
    "            TP = CM_KNN[1,1]\n",
    "            TN = CM_KNN[2,2]\n",
    "            FP = CM_KNN[2,1]\n",
    "            FN = CM_KNN[1,2]\n",
    "\n",
    "            accuracy_expected = (TP + TN) / sum(CM_KNN)\n",
    "            precision_expected = (TP + FP) == 0 ? 0.0 : TP / (TP + FP)\n",
    "            recall_expected = (TP + FN) == 0 ? 0.0 : TP / (TP + FN)\n",
    "            specificity_expected = (TN + FP) == 0 ? 0.0 : TN / (TN + FP)\n",
    "            balanced_accuracy_expected = 0.5 * (recall_expected + specificity_expected)\n",
    "            f1_expected = (precision_expected + recall_expected) == 0 ? 0.0 : 2 * precision_expected * recall_expected / (precision_expected + recall_expected)\n",
    "\n",
    "            @test 0.0 <= accuracy_KNN <= 1.0\n",
    "            @test 0.0 <= precision_KNN <= 1.0\n",
    "            @test 0.0 <= recall_KNN <= 1.0\n",
    "            @test 0.0 <= balanced_accuracy_KNN <= 1.0\n",
    "            @test 0.0 <= f1_KNN <= 1.0\n",
    "            @test isapprox(accuracy_KNN, accuracy_expected, atol=1e-12)\n",
    "            @test isapprox(precision_KNN, precision_expected, atol=1e-12)\n",
    "            @test isapprox(recall_KNN, recall_expected, atol=1e-12)\n",
    "            @test isapprox(balanced_accuracy_KNN, balanced_accuracy_expected, atol=1e-12)\n",
    "            @test isapprox(f1_KNN, f1_expected, atol=1e-12)\n",
    "        end\n",
    "\n",
    "        @testset \"Task 3: Hyperparameter Sweep\" begin\n",
    "            @test isnothing(knn_sweep_results) == false\n",
    "            @test nrow(knn_sweep_results) > 0\n",
    "            @test names(knn_sweep_results) == [\"K\", \"gamma\", \"TP\", \"TN\", \"FP\", \"FN\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"balanced_accuracy\"]\n",
    "            @test issorted(knn_sweep_results.f1, rev=true)\n",
    "            @test all(knn_sweep_results.K .> 0)\n",
    "            @test all(isodd.(knn_sweep_results.K))\n",
    "            @test all(knn_sweep_results.gamma .> 0.0)\n",
    "            @test all((knn_sweep_results.TP .+ knn_sweep_results.TN .+ knn_sweep_results.FP .+ knn_sweep_results.FN) .== number_of_tests_to_run)\n",
    "            @test all((0.0 .<= knn_sweep_results.accuracy) .& (knn_sweep_results.accuracy .<= 1.0))\n",
    "            @test all((0.0 .<= knn_sweep_results.precision) .& (knn_sweep_results.precision .<= 1.0))\n",
    "            @test all((0.0 .<= knn_sweep_results.recall) .& (knn_sweep_results.recall .<= 1.0))\n",
    "            @test all((0.0 .<= knn_sweep_results.f1) .& (knn_sweep_results.f1 .<= 1.0))\n",
    "            @test all((0.0 .<= knn_sweep_results.balanced_accuracy) .& (knn_sweep_results.balanced_accuracy .<= 1.0))\n",
    "\n",
    "            for row in eachrow(knn_sweep_results)\n",
    "                TP = row.TP\n",
    "                TN = row.TN\n",
    "                FP = row.FP\n",
    "                FN = row.FN\n",
    "\n",
    "                accuracy_expected = (TP + TN) / (TP + TN + FP + FN)\n",
    "                precision_expected = (TP + FP) == 0 ? 0.0 : TP / (TP + FP)\n",
    "                recall_expected = (TP + FN) == 0 ? 0.0 : TP / (TP + FN)\n",
    "                specificity_expected = (TN + FP) == 0 ? 0.0 : TN / (TN + FP)\n",
    "                balanced_accuracy_expected = 0.5 * (recall_expected + specificity_expected)\n",
    "                f1_expected = (precision_expected + recall_expected) == 0 ? 0.0 : 2 * precision_expected * recall_expected / (precision_expected + recall_expected)\n",
    "\n",
    "                @test isapprox(row.accuracy, accuracy_expected, atol=1e-12)\n",
    "                @test isapprox(row.precision, precision_expected, atol=1e-12)\n",
    "                @test isapprox(row.recall, recall_expected, atol=1e-12)\n",
    "                @test isapprox(row.balanced_accuracy, balanced_accuracy_expected, atol=1e-12)\n",
    "                @test isapprox(row.f1, f1_expected, atol=1e-12)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        @testset \"Discussion Questions\" begin\n",
    "            @test did_I_answer_DQ1 == true\n",
    "            @test did_I_answer_DQ2 == true\n",
    "            @test did_I_answer_DQ3 == true\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
