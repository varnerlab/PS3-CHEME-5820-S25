{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebe7831",
   "metadata": {},
   "source": [
    "# PS3: Let's Classify RNA data using K-Nearest Neighbors (KNN)\n",
    "In this problem set, we will implement the K-Nearest Neighbors (KNN) algorithm to classify RNA data. \n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> By the end of this problem set, you should be able to:\n",
    "> Three learning objectives here\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabe961",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> __Environment Setup with Include.jl__\n",
    ">\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/).\n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9ee499",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726cd62b",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1261c",
   "metadata": {},
   "source": [
    "### Data\n",
    "Let's load [a dataset from the `LIBSVM` data archive](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) that describes the detection of non-coding RNA sequences that was initially published by:\n",
    "* [Andrew V Uzilov, Joshua M Keegan, and David H Mathews. Detection of non-coding RNAs on the basis of predicted secondary structure formation free energy change. BMC Bioinformatics, 7(173), 2006.](https://pubmed.ncbi.nlm.nih.gov/16566836/)\n",
    "\n",
    "\n",
    "Non-coding RNAs (ncRNAs) have many roles in cells. However, detecting novel ncRNAs in biochemical screens is challenging. Accurate computational methods for detecting ncRNAs in sequenced genomes are important to understanding the roles ncRNAs play in cells. \n",
    "\n",
    "> __What's in the dataset?__\n",
    "> \n",
    "> In this dataset, there are `59535` training instances in the `training` data; each instance has `8` continuous features and a binary label $y\\in\\left\\{-1,1\\right\\}$. The `test` dataset has `271617` instances (with the same `8` continuous features and a binary label).\n",
    "\n",
    "We begin by loading the `training` and `test` datasets. The [`LIBSVM` library authors](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) have developed these subsets. \n",
    "\n",
    "However, we'll need to preprocess both the `training` and `test` sets. Before we do this, we'll set some constants. Please look at the comment next to the constant for a definition of what it is, units, permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0241898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 8; # there are eight continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f750f3",
   "metadata": {},
   "source": [
    "In the code block below we preprocess the `training` dataset. We have [z-score centered](https://en.wikipedia.org/wiki/Standard_score) the training data and combined it into an array where each row is a training instance, while the first `1:number_of_features` columns hold the features. The last column has the label. \n",
    "\n",
    "We store the training data in the `training::Array{NamedTuple,1}` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033a7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = let\n",
    "\n",
    "    # load the training data -\n",
    "    data = parser(joinpath(_PATH_TO_DATA, \"cod-rna-training.data\"));\n",
    "    number_of_rows = size(data,1);\n",
    "    data_perm = randperm(number_of_rows);\n",
    "    training_data = Array{NamedTuple,1}(undef, number_of_rows);\n",
    "    X = data[data_perm,:];\n",
    "    \n",
    "    # z-score center the data -\n",
    "    μ = mean(X[:,1:number_of_features],dims=1);\n",
    "    σ = std(X[:,1:number_of_features],dims=1);\n",
    "    X̂ = zeros(number_of_rows,number_of_features+1);\n",
    "    for i ∈ 1:number_of_rows\n",
    "        for j ∈ 1:number_of_features\n",
    "            X̂[i,j] = (X[i,j] - μ[j])/(σ[j]);\n",
    "        end\n",
    "        X̂[i,end] = X[i,end]; # get the label\n",
    "    end\n",
    "\n",
    "    # package the data into an array of named tuples -\n",
    "    for i ∈ 1:number_of_rows\n",
    "        features = X̂[i,1:number_of_features];\n",
    "        label = X̂[i,end];\n",
    "        training_data[i] = (x = features, y = (label |> Int ));\n",
    "    end\n",
    "\n",
    "    training_data; # return scaled - balanced data\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd5436",
   "metadata": {},
   "source": [
    "Next, we preprocess the `test` dataset.  We [z-score center](https://en.wikipedia.org/wiki/Standard_score) the test data and combined it into an array where each row is a test instance, while the first `1:number_of_features` columns hold the features. The last column has the label.\n",
    "\n",
    "We store the training data in the `test::Array{NamedTuple,1}` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b067bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = let\n",
    "\n",
    "    # load the training data -\n",
    "    data = parser(joinpath(_PATH_TO_DATA, \"cod-rna-testing.data\"));\n",
    "    number_of_rows = size(data,1);\n",
    "    data_perm = randperm(number_of_rows);\n",
    "    test_data = Array{NamedTuple,1}(undef, number_of_rows);\n",
    "    X = data[data_perm,:];\n",
    "    \n",
    "    # z-score center the data -\n",
    "    μ = mean(X[:,1:number_of_features],dims=1);\n",
    "    σ = std(X[:,1:number_of_features],dims=1);\n",
    "    X̂ = zeros(number_of_rows,number_of_features+1);\n",
    "    for i ∈ 1:number_of_rows\n",
    "        for j ∈ 1:number_of_features\n",
    "            X̂[i,j] = (X[i,j] - μ[j])/(σ[j]);\n",
    "        end\n",
    "        X̂[i,end] = X[i,end]; # get the label\n",
    "    end\n",
    "\n",
    "    # package the data into an array of named tuples -\n",
    "    for i ∈ 1:number_of_rows\n",
    "        features = X̂[i,1:number_of_features];\n",
    "        label = X̂[i,end];\n",
    "        test_data[i] = (x=features, y = (label |> Int ));\n",
    "    end\n",
    "\n",
    "    test_data; # return scaled - balanced data\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f38bb0",
   "metadata": {},
   "source": [
    "__Are the labels in the training dataset balanced?__ Since we are using a KNN classifier, it is important to have balanced labels in the training dataset. If the labels are imbalanced in our reference dataset, the KNN classifier may be biased towards the majority class, which can lead to poor performance on the minority class. We can check for label balance by looking at the distribution of labels in the training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224314eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of positive labels in the training dataset: 0.3333333333333333\n",
      "Fraction of negative labels in the training dataset: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "let \n",
    "\n",
    "    # initialize -\n",
    "    D = training; # specify what data set we are working with\n",
    "    number_of_training_examples = length(D); # how many examples are in the training dataset?\n",
    "\n",
    "    # Fancy! Let's use a list comprehension to count the number of positive and negative labels in the training dataset. The `sum()` function will sum up the number of times the condition is true for each example in the training dataset.\n",
    "    count_positive_labels = sum(D[i].y == 1 for i ∈ 1:number_of_training_examples); # how many positive labels are in the training dataset?\n",
    "    count_negative_labels = sum(D[i].y == -1 for i ∈ 1:number_of_training_examples); # how many negative labels are in the training dataset?\n",
    "    \n",
    "    # print the results (fraction of positive and negative labels in the training dataset)\n",
    "    println(\"Fraction of positive labels in the training dataset: \", count_positive_labels / number_of_training_examples);\n",
    "    println(\"Fraction of negative labels in the training dataset: \", count_negative_labels / number_of_training_examples);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ae6de",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfa5a2",
   "metadata": {},
   "source": [
    "## Task 2: Let's look at our KNN Classifier\n",
    "In this task, we will build and evaluate the KNN classifier that we will be using to make predictions on the test dataset. We'll build [a `MyWeightedKernelizedKNNClassificationModel` model](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.MyWeightedKernelizedKNNClassificationModel) using the `training` dataset as the reference data.\n",
    "\n",
    "Let's build a kernel function $k:\\mathbb{R}^{m}\\times\\mathbb{R}^{m}\\to\\mathbb{R}$ to measure similarity. For now, let's make up our own kernel function, and save this function in the `k(x,y)::Function` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa8a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k(x,y,γ) = exp(-γ * norm(x-y,2)^2) # RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56aaf7",
   "metadata": {},
   "source": [
    "#### Check: Are we using a valid Kernel function?\n",
    "Let's check to see if the distance (similarity) metric we built is a valid kernel function.\n",
    "\n",
    "> __Condition:__\n",
    ">\n",
    "> A function $k:\\mathbb{R}^{m}\\times\\mathbb{R}^{m}\\to\\mathbb{R}$ is a _valid kernel function_ if and only if the kernel matrix $\\mathbf{K}\\in\\mathbb{R}^{n\\times{n}}$ is positive (semi)definite for all possible choices of the data vectors $\\mathbf{v}_i$, where $K_{ij} = k(\\mathbf{v}_i, \\mathbf{v}_j)$. If $\\mathbf{K}$ is positive (semi)definite, then for any real-valued vector $\\mathbf{x} \\in \\mathbb{R}^n$, the kernel matrix $\\mathbf{K}$ must satisfy $\\mathbf{x}^{\\top}\\mathbf{K}\\mathbf{x} \\geq 0$. \n",
    "\n",
    "Let's compute the kernel matrix `KM::Array{Float64,2}` for a data matrix `X::Array{Float64,2}` using the distance/kernel function `k(x,y)::Function` we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfa6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "KM = let\n",
    "\n",
    "    D = training; # specify what data set we are working with\n",
    "    number_of_training_examples = 1000; # how many examples are in the training dataset (we will use only small number examples to compute the kernel matrix for computational efficiency)\n",
    "    number_of_features = D[1].x |> length; # number of features in the dataset\n",
    "    γ = 0.5; # kernel parameter\n",
    "\n",
    "    # fill up the feature matrix -\n",
    "    X = zeros(number_of_training_examples, number_of_features); # initialize a matrix to hold the features\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        X[i,:] = D[i].x; # fill the matrix with the features from the training dataset\n",
    "    end\n",
    "\n",
    "    # fill up the kernel matrix -\n",
    "    K = zeros(number_of_training_examples,number_of_training_examples);\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        vᵢ = X[i,:];\n",
    "        for j ∈ 1:number_of_training_examples\n",
    "            vⱼ = X[j,:];\n",
    "            K[i,j] = k(vᵢ,vⱼ,γ) # compute kernel value\n",
    "        end\n",
    "    end\n",
    "    K\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115937a2",
   "metadata": {},
   "source": [
    "Next, let's check to see if the kernel matrix `K::Array{Float64,2}` is positive (semi)definite by checking if all of its eigenvalues are non-negative.\n",
    "\n",
    "> __Check:__\n",
    ">\n",
    "> For this kernel to be valid, the kernel matrix $\\mathbf{K}$ needs to be positive (semi)definite, i.e., all eigenvalues $\\lambda_i \\geq 0$. We compute the eigenvalues using [`eigvals`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigvals) and verify they are all non-negative using [the `@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) in combination with [the `all` function](https://docs.julialang.org/en/v1/base/collections/#Base.all-Tuple%7BAny%7D).\n",
    "\n",
    "Do we blow up? If not, the matrix is PSD for this dataset, which supports using this kernel in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae360c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    λ = eigvals(K);\n",
    "    @assert all(λ .≥ -1e-10) \"Kernel matrix is not PSD: min eigenvalue = $(minimum(λ))\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d9ace",
   "metadata": {},
   "source": [
    "## Summary\n",
    "One concise summary sentence of the problem set here.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> Three key takeaways here\n",
    "\n",
    "One direct, concise conclusion sentence here.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.5",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
