{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32fe3c6d-09ea-4c0e-81f7-3c8454d05e77",
   "metadata": {},
   "source": [
    "# PS3: Kernelized Support Vector Machines (kSVMs)\n",
    "In this problem set, we will experiment with a kernel Support Vector Machine (SVM) to classify challenging non-linearly separable datasets taken from the literature. In particular, we'll look at a kernelized version of the $\\nu$-soft-margin support vector machine. If these terms are unfamiliar, [check out the L5c notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-5/L5c/docs/Notes.pdf) and the [this review](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf).\n",
    "\n",
    "### Theory: Support Vector Machine (SVM)\n",
    "Suppose we have dataset $\\mathcal{D} = \\{(\\hat{\\mathbf{x}}_{i}, y_{i}) \\mid i = 1,2,\\dots,n\\}$, where $\\hat{\\mathbf{x}}_i \\in \\mathbb{R}^p$ is an _augmented_ feature vector ($m$ features with additional `1` to model the bias on the end of the vector) and $y_i \\in \\{-1, 1\\}$ is the corresponding class label. The goal of an SVM (for binary classification tasks) is to find the hyperplane $\\mathcal{H}(\\hat{\\mathbf{x}}) = \\{\\hat{\\mathbf{x}} \\mid \\left<\\hat{\\mathbf{x}},\\theta\\right> = 0\\}$ that separates the data points into two classes (those points above the hyperplane, and those points below the hyperplane), where $\\theta \\in \\mathbb{R}^{p}$ ($p=m+1$) is the normal vector to the hyperplane, or the parameters of the model that we need to estimate.\n",
    "\n",
    "#### $\\nu$-Soft margin\n",
    "Today, we'll use a variant of the soft-margin support vector machine developed by (...) called the $\\nu$-support vector classifier developed by the group:\n",
    "* [Scholkopf B, Smola AJ, Williamson RC, Bartlett PL. New support vector algorithms. Neural Comput. 2000 May;12(5):1207-45. doi: 10.1162/089976600300015565. PMID: 10905814.](https://pubmed.ncbi.nlm.nih.gov/10905814/)\n",
    "\n",
    "This approach introduces a new parameter $\\nu\\in(0,1]$, where $\\nu$ approximates the upper bound on the number of training mistakes. In this variant, the training problem is defined as: \n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min_{\\theta}\\quad & \\frac{1}{2}\\lVert{\\theta}\\rVert_{2}^{2} + C\\sum_{i=1}^{n}\\xi_{i}\\\\\n",
    "    \\text{subject to}\\quad & y_{i}\\left<\\hat{\\mathbf{x}}_{i},\\theta\\right> \\geq 1 - \\xi_{i}\\quad\\forall i\\\\\n",
    "    & \\xi_{i} \\geq 0\\quad\\forall i\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\xi_{i}$ is a _slack variable_, that quantifies the cost of a classification mistake, and $C>{0}$ is a user-adjustable parameter that controls the trade-off between maximizing the margin and minimizing the slack variables.\n",
    "* __Values of $C$__: If $C\\gg{1}$ the classifier will behave like the maximum (hard) margin classifier, i.e., mistakes will be expensive, and the search will avoid making choices with mistakes. However, if $C\\ll{1}$, the classifier will allow more slack (mistakes), i.e., mistakes are cheap, so what's it matter?\n",
    "\n",
    "### Tasks\n",
    "Before we start, divide into teams and familiarize yourself with the lab. Then, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "\n",
    "* __Task 1: Setup, Data, Constants (10 min)__: Let's take 10 minutes to explore how we will generate the datasets we'll explore today. We'll work through how to generate linearly separable and non-linearly separable datasets.\n",
    "* __Task 2: Linear SVM classification (15 min)__: In this task, we [use the SVM implementation exported by the `LIBSVM.jl` package](https://github.com/JuliaML/LIBSVM.jl) to classify the dataset $\\mathcal{D}$ generated in task 1 using a `linear kernel`. In particular, we use the `training` dataset to estimate the unknown model parameters $\\theta$ [using the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl), and the `test` data to evaluate the performance of the classifier on unseen data [using the `svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl).\n",
    "* __Task 3: Implement a Grid Search to estimate the optimal hyperparameters for an RBF-SVM (15 min)__: In this task, we'll perform a grid search to estimate the best hyperparameters for a keneralized SVM using the RBF kernel. We'll estimate the best $C$ parameter in the objective function and the length-scale $\\gamma$ parameter.\n",
    "\n",
    "Let's get going!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef21be-0201-4515-89bb-ba8112541d8e",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070be284-e10c-4c83-90bb-4cab39d97ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be56e42-f2dd-4da0-a8df-b1bae63621eb",
   "metadata": {},
   "source": [
    "### Data\n",
    "In this section, we'll load [a dataset from the `LIBSVM` data archive](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) that describes the detection on non-coding RNA sequence initially published by:\n",
    "* [Andrew V Uzilov, Joshua M Keegan, and David H Mathews. Detection of non-coding RNAs on the basis of predicted secondary structure formation free energy change.\n",
    "BMC Bioinformatics, 7(173), 2006.](https://pubmed.ncbi.nlm.nih.gov/16566836/)\n",
    "\n",
    "\n",
    "Non-coding RNAs (ncRNAs) have many roles in cells. Detecting novel ncRNAs in biochemical screens is challenging. Accurate computational methods for detecting ncRNAs in sequenced genomes are necessary to advance biological knowledge. The growing number of genomic sequences offers a rich dataset for comparative sequence analysis and novel ncRNA detection. \n",
    "\n",
    "* There are `59535` training instances in the `training` data, each instance has `8` continuous features and a binary label $y\\in\\left\\{-1,1\\right\\}$. The `test` dataset has `271617` instances (with the same `8` continuous features and a binary label).\n",
    "\n",
    "Load the `training` and `test` datasets. The [`LIBSVM` library authors](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/) have developed these subsets. However, we'll need to preprocess both subsets as described below. Before we do this, we'll set some constants. See the comment next to the constant for a definition of what it is, units, permissible values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ee4e127-97af-42d7-bd51-99e8897aa76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = 10000; # has to be even, less than the \n",
    "number_of_test_examples = 1000; # has to be even\n",
    "number_of_features = 8;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a38761-a24e-45a9-b02d-4fde1fb51963",
   "metadata": {},
   "source": [
    "`Unhide` the code block below to see how we preprocessed the `training` dataset. We have [z-score centered](https://en.wikipedia.org/wiki/Standard_score) the training data. Each row is a training instance, while the first `1:number_of_features` columns hold the features. The last column has the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4099b294-ce46-414c-98b4-69ea85e3743a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training = let\n",
    "\n",
    "    # load the training data -\n",
    "    data = parser(joinpath(_PATH_TO_DATA, \"cod-rna-training.data\"));\n",
    "    number_of_rows = size(data,1);\n",
    "    data_perm = range(1,stop=number_of_rows,step=1) |> collect |> i-> shuffle!(i);\n",
    "    X = data[data_perm,:];\n",
    "    \n",
    "    # z-score center the data -\n",
    "    μ = mean(X[:,1:number_of_features],dims=1);\n",
    "    σ = std(X[:,1:number_of_features],dims=1);\n",
    "    X̂ = zeros(number_of_rows,number_of_features+1);\n",
    "    for i ∈ 1:number_of_rows\n",
    "        for j ∈ 1:number_of_features\n",
    "            X̂[i,j] = (X[i,j] - μ[j])/(σ[j]);\n",
    "        end\n",
    "        X̂[i,end] = X[i,end]; # get the label\n",
    "    end\n",
    "\n",
    "    # Finally, let's make sure the labels are balanced -\n",
    "    yₒ = 1;\n",
    "    is_ok_to_loop = true;\n",
    "    tmp = Set{Array{Float64,1}}();\n",
    "    while (is_ok_to_loop == true)\n",
    "        i = rand(1:number_of_rows); # generate a random index\n",
    "        y = X̂[i,end]\n",
    "        if (y == yₒ)\n",
    "            x = X̂[i,:];\n",
    "            push!(tmp,x);\n",
    "        end\n",
    "\n",
    "        if (length(tmp) ≥ round(number_of_training_examples/2))\n",
    "            is_ok_to_loop = false; # stop\n",
    "        end\n",
    "    end\n",
    "\n",
    "    yₒ = -1;\n",
    "    is_ok_to_loop = true;\n",
    "    while (is_ok_to_loop == true)\n",
    "        i = rand(1:number_of_rows); # generate a random index\n",
    "        y = X̂[i,end]\n",
    "        if (y == yₒ)\n",
    "            x = X̂[i,:];\n",
    "            push!(tmp,x);\n",
    "        end\n",
    "\n",
    "        if (length(tmp) ≥ number_of_training_examples)\n",
    "            is_ok_to_loop = false; # stop\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    D = Array{Float64,2}(undef, number_of_training_examples, (number_of_features + 1));\n",
    "    for i ∈ 1:number_of_training_examples\n",
    "        x = pop!(tmp);\n",
    "        for j ∈ 1:(number_of_features + 1)\n",
    "            D[i,j] = x[j];\n",
    "        end\n",
    "    end\n",
    "\n",
    "    D; # return scaled - balanced data\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd427f-508a-42f0-a15e-0590a3ea3206",
   "metadata": {},
   "source": [
    "`Unhide` the code block below to see how we preprocessed the `test` dataset.  We have [z-score centered](https://en.wikipedia.org/wiki/Standard_score) the test data. Each row is a testing instance, while the first `1:number_of_features` columns hold the features. The last column has the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93abdbf4-aac4-43c0-ae82-afcf1ffad359",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test = let\n",
    "\n",
    "    data = parser(joinpath(_PATH_TO_DATA, \"cod-rna-testing.data\"));\n",
    "    number_of_rows = size(data,1);\n",
    "    data_perm = range(1,stop=number_of_rows,step=1) |> collect |> i-> shuffle!(i);\n",
    "    X = data[data_perm,:];\n",
    "    \n",
    "    # z-score center the data -\n",
    "    μ = mean(X[:,1:number_of_features],dims=1);\n",
    "    σ = std(X[:,1:number_of_features],dims=1);\n",
    "    X̂ = zeros(number_of_rows,number_of_features+1);\n",
    "    for i ∈ 1:number_of_rows\n",
    "        for j ∈ 1:number_of_features\n",
    "            X̂[i,j] = (X[i,j] - μ[j])/(σ[j]);\n",
    "        end\n",
    "        X̂[i,end] = X[i,end]; # get the label\n",
    "    end\n",
    "\n",
    "    # Finally, let's make sure the labels are balanced -\n",
    "    yₒ = 1;\n",
    "    is_ok_to_loop = true;\n",
    "    tmp = Set{Array{Float64,1}}();\n",
    "    while (is_ok_to_loop == true)\n",
    "        i = rand(1:number_of_rows); # generate a random index\n",
    "        y = X̂[i,end]\n",
    "        if (y == yₒ)\n",
    "            x = X̂[i,:];\n",
    "            push!(tmp,x);\n",
    "        end\n",
    "\n",
    "        if (length(tmp) ≥ round(number_of_test_examples/2))\n",
    "            is_ok_to_loop = false; # stop\n",
    "        end\n",
    "    end\n",
    "\n",
    "    yₒ = -1;\n",
    "    is_ok_to_loop = true;\n",
    "    while (is_ok_to_loop == true)\n",
    "        i = rand(1:number_of_rows); # generate a random index\n",
    "        y = X̂[i,end]\n",
    "        if (y == yₒ)\n",
    "            x = X̂[i,:];\n",
    "            push!(tmp,x);\n",
    "        end\n",
    "\n",
    "        if (length(tmp) ≥ number_of_test_examples)\n",
    "            is_ok_to_loop = false; # stop\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    D = Array{Float64,2}(undef, number_of_test_examples, (number_of_features + 1));\n",
    "    for i ∈ 1:number_of_test_examples\n",
    "        x = pop!(tmp);\n",
    "        for j ∈ 1:(number_of_features + 1)\n",
    "            D[i,j] = x[j];\n",
    "        end\n",
    "    end\n",
    "\n",
    "    D; # return scaled - balanced data\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ff19e-5964-4fc5-90c8-0f0577ba06bc",
   "metadata": {},
   "source": [
    "## Task 2: Classification using an Linear SVM\n",
    "In this task, we [use the SVM implementation exported by the `LIBSVM.jl` package](https://github.com/JuliaML/LIBSVM.jl) to classify the dataset $\\mathcal{D}$ generated in task 1 using a `linear kernel`. In particular, we use the `training` dataset to estimate the unknown model parameters $\\theta$ [using the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl), and the `test` data to evaluate the performance of the classifier on unseen data [using the `svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) takes an augmented training examples matrix $\\hat{\\mathbf{X}}^{\\top}$ where the examples are on the columns and the features are the rows, and a label vector $\\mathbf{y}\\in\\left\\{-1,1\\right\\}$.\n",
    "* The [`svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns a [model instance](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) that holds the trained data and a bunch of other data associated with the problem.\n",
    "* __Hmmm__: One of the (super) interesting optional arguments [the `svmtrain(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) is the `kernel` argument. Check out the documentation to see what kernels are supported! Wow! we get [kernelized SVM capability](https://en.wikipedia.org/wiki/Support_vector_machine#Nonlinear_kernels) right out of the box. _Buy versus build, 99% buy!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c5e427-8dce-47b2-a1d3-51fd7bcfbbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "\n",
    "    # Setup the data that we are using\n",
    "    D = training; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "\n",
    "    # TODO: Uncomment the line below to train the SVM model using the training data \n",
    "    model = svmtrain(X, y, kernel=LIBSVM.Kernel.Linear, verbose = false, svmtype = LIBSVM.NuSVC); # we are using the LIBSVM\n",
    "\n",
    "    # return\n",
    "    model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a40341-4c8c-4511-8a6f-814182d10735",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between an actual banknote and a forgery on data it has never seen. We run the classification operation on the (unseen) test data [using the `svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl). \n",
    "* The [`svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) returns the predicted label which we store in the `ŷ::Array{Int64,1}` array. We store the actual (correct) label in the `y::Array{Int64,1}` vector.\n",
    "* The [`svmpredict(...)` method](https://github.com/JuliaML/LIBSVM.jl/blob/master/src/LIBSVM.jl) also returns a second output which we save in the `decision_values` variable. __Hmmmm__. Not sure what these values are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b42d7969-9bfa-4f4f-98fb-ea9443c11eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ,y,d = let\n",
    "\n",
    "     # Setup the data that we are using\n",
    "    D = test; # what dataset are we looking at?\n",
    "    number_of_examples = size(D,1); # how many rows?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y = D[:,end]; # label\n",
    "    \n",
    "    # TODO: Uncomment the line below to test the SVM model on the other block of the data.\n",
    "    ŷ, decision_values = svmpredict(model, X);\n",
    "\n",
    "    # return -\n",
    "    ŷ,y,decision_values\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea4db2-103c-4e8f-b858-96dd1fb66db5",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Finally, let's compute the confusion matrix. The confusion matrix is a $2\\times{2}$ matrix that contains four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). [Click me for a confusion matrix schematic!](https://github.com/varnerlab/CHEME-5820-Labs-Spring-2025/blob/main/labs/week-3/L3b/figs/Fig-BinaryConfusionMatrix.pdf). Let's compute these four values [using the `confusion(...)` method](src/Compute.jl) and store them in the `CM::Array{Int64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57ea7ec-1fd8-41cf-9d4d-e85768e10d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Int64}:\n",
       " 395  105\n",
       " 400  100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM = confusion(y, ŷ) # call with the SVM test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52750870-9c80-4e72-8dfa-6761ba710171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct: 0.495 Fraction incorrect 0.505\n"
     ]
    }
   ],
   "source": [
    "number_of_test_points = length(y);\n",
    "correct_prediction_perceptron = CM[1,1] + CM[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e2021-5686-473e-8f6b-a4508bb14a31",
   "metadata": {},
   "source": [
    "## Task 3: Implement a Grid Search to estimate the optimal hyperparameters for an RBF-SVM\n",
    "In this task, we'll perform a grid search to estimate the best hyperparameters for a keneralized SVM using the RBF kernel. We'll estimate the best $C$ parameter in the objective function and the length-scale $\\gamma$ parameter.\n",
    "\n",
    "[A grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) for kernel SVM parameters C and $\\gamma$ involves systematically exploring combinations of these hyperparameters to find the optimal configuration for model performance. Here's a description of the process:\n",
    "* __Define parameter ranges__. For the cost parameter $C$, we use $C\\in\\left\\{2^{-5},2^{-3},\\dots,2^{15}\\right\\}$ while for the length scale parameter $\\gamma$, we use $\\gamma\\in\\left\\{2^{-15},2^{-13},\\dots,2^{3}\\right\\}$. We store the exponents of these ranges in the `α::Array{Float64,1}` and `β::Array{Float64,1}` arrays, respectively.\n",
    "* __Model training and evaluation__: For each parameter combination $(C_{i},\\gamma_{j})$ we train a SVM model with a RBF kernel, compute the confusion matrix and then evaluate the prediction accuracy. We save the accuracy data in the `A::Array{Float64,2}` array, where $a_{ij}\\in\\mathbf{A}$ holds the accuracy values for the parameter combination $(C_{i},\\gamma_{j})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "916a6ac0-a18f-4b0e-9aa5-dda1e0fa7dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1\n",
      "i = 2\n",
      "i = 3\n",
      "i = 4\n",
      "i = 5\n"
     ]
    }
   ],
   "source": [
    "A, α, β = let\n",
    "\n",
    "    # Training data setup -\n",
    "    D₁ = training; # what dataset are we looking at?\n",
    "    number_of_training_examples = size(D₁,1); # how many rows?\n",
    "    X₁ = [D₁[:,1:end-1] ones(number_of_training_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y₁ = D₁[:,end]; # label\n",
    "\n",
    "    # Test data setup -\n",
    "    D₂ = test; # what dataset are we looking at?\n",
    "    number_of_test_examples = size(D₂,1); # how many rows?\n",
    "    X₂ = [D₂[:,1:end-1] ones(number_of_test_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y₂ = D₂[:,end]; # label\n",
    "    \n",
    "    α = range(0.1,stop = 0.9, step=0.2) |> collect; # exponent for nu -\n",
    "    β = range(-15,stop = 4, step=2) |> collect; # exponent for γ -\n",
    "    number_of_points_C = length(α);\n",
    "    number_of_points_gamma = length(β);\n",
    "    accuracy = Array{Float64,2}(undef, number_of_points_C, number_of_points_gamma);\n",
    "    \n",
    "    for i ∈ eachindex(α)\n",
    "        nu = α[i];\n",
    "        for j ∈ eachindex(β)\n",
    "            γ = 2.0^β[j];\n",
    "\n",
    "            # TODO: Uncomment below to train the mode in the (C,γ) values -\n",
    "            ŷ₂,_ = svmtrain(X₁, y₁, kernel=LIBSVM.Kernel.RadialBasis, \n",
    "                verbose = false, nu = nu, gamma = γ, nt = -1, svmtype = LIBSVM.NuSVC) |> model -> svmpredict(model,X₂);\n",
    "\n",
    "            # how many mistakes?\n",
    "            accuracy[i,j] = confusion(y₂, ŷ₂) |> CM -> CM[1,1] + CM[2,2] |> correct -> correct/number_of_test_examples;\n",
    "        end\n",
    "        @show i\n",
    "    end\n",
    "    \n",
    "    accuracy, α, β\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfccd256-16f9-4a9f-a649-f25f161fdb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJYAAABLCAAAAAC3yiHPAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAANxJREFUaAXNwTGSE1EAxUD9sUzBqffYW2AYE7/Esbr9YlyMm3EYL8aTcTMejCfjYkiSJEmSJEmSJEmSJEmSJEmSJEmSJHkzHnxyM56MB+NiHMZhXAxJkiRJkiRJkiRJkiRJkiRJkiRJkiQP4zAO4834xziMN+MwDp9IkiRJkiRJkiRJkiRJkiRJkiRJkiR5M74Zvxgvxs04jAfjzbgYhyFJkiRJkiRJkiRJkiRJkiRJkiRJkuQPxh/GXz45jCfjZrwYvxk/GZIkSZIkSZIkSZIkSZIkSZIkSZIkSf8BaAcTvHoDkOYAAAAASUVORK5CYII=",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJYAAABLCAAAAAC3yiHPAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAANxJREFUaAXNwTGSE1EAxUD9sUzBqffYW2AYE7/Esbr9YlyMm3EYL8aTcTMejCfjYkiSJEmSJEmSJEmSJEmSJEmSJEmSJHkzHnxyM56MB+NiHMZhXAxJkiRJkiRJkiRJkiRJkiRJkiRJkiQP4zAO4834xziMN+MwDp9IkiRJkiRJkiRJkiRJkiRJkiRJkiR5M74Zvxgvxs04jAfjzbgYhyFJkiRJkiRJkiRJkiRJkiRJkiRJkuQPxh/GXz45jCfjZrwYvxk/GZIkSZIkSZIkSZIkSZIkSZIkSZIkSf8BaAcTvHoDkOYAAAAASUVORK5C\">"
      ],
      "text/plain": [
       "5×10 Matrix{Gray{Float64}}:\n",
       " 0.497  0.504  0.497  0.503  0.481  0.501  0.496  0.506  0.526  0.534\n",
       " 0.489  0.508  0.498  0.494  0.501  0.512  0.511  0.514  0.53   0.543\n",
       " 0.495  0.512  0.503  0.494  0.491  0.507  0.503  0.505  0.534  0.542\n",
       " 0.488  0.48   0.512  0.482  0.475  0.496  0.505  0.503  0.542  0.544\n",
       " 0.508  0.487  0.499  0.481  0.479  0.512  0.507  0.482  0.512  0.544"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gray.(1 .- A) # fun! More accurate parameter combinations are darker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "476dbf34-3f9d-482e-a389-46b0325d6872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×10 Matrix{Float64}:\n",
       " 0.503  0.496  0.503  0.497  0.519  0.499  0.504  0.494  0.474  0.466\n",
       " 0.511  0.492  0.502  0.506  0.499  0.488  0.489  0.486  0.47   0.457\n",
       " 0.505  0.488  0.497  0.506  0.509  0.493  0.497  0.495  0.466  0.458\n",
       " 0.512  0.52   0.488  0.518  0.525  0.504  0.495  0.497  0.458  0.456\n",
       " 0.492  0.513  0.501  0.519  0.521  0.488  0.493  0.518  0.488  0.456"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A # rows are C, cols are γ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00c9e8-1f2c-4e93-b766-cd6b634ae35d",
   "metadata": {},
   "source": [
    "### What is the best SVM model?\n",
    "Let's find the model with the highest training accuracy. We'll call this the _best model_ and save it in the `best_model::LIBSVM.SVM` variable. First, which element of the accuracy matrix $\\mathbf{A}$ holds the maximum?\n",
    "* We can estimate maximum accuracy element of the matrix $\\mathbf{A}$ [using the `maximum(...)` method](https://docs.julialang.org/en/v1/base/collections/#Base.maximum). The `(i,j)` position of the maximum element can be computed using [the `argmax(...)` method](https://docs.julialang.org/en/v1/base/collections/#Base.argmax). The [`argmax(...)` method](https://docs.julialang.org/en/v1/base/collections/#Base.argmax) returns a cool data structure [called a `CartesianIndex`](https://docs.julialang.org/en/v1/base/arrays/#Base.IteratorsMD.CartesianIndex) which holds the (`row, col`) values of the maximum. This data structure is a way to model collection indices (which seems interesting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b7cb67f-dcaa-4fcd-a800-389d8b580625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test accuracy: 0.525\n"
     ]
    }
   ],
   "source": [
    "coordinate = argmax(A)\n",
    "best_accuracy = maximum(A)\n",
    "println(\"Best test accuracy: $(best_accuracy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1269ae-8a2f-4720-8b23-d96b06198aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CartesianIndex(4, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f19ea-d180-491e-bde0-6092861dac79",
   "metadata": {},
   "source": [
    "Next, get the best parameters, and save these in the `C_best::Float64` and `γ_best::Float64` variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab4c8dd4-b872-413b-852c-4f0e578bdfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.0078125)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_best, γ_best = let\n",
    "    \n",
    "    C = coordinate[1] |> i-> α[i]; # Wow! we grab the row (corresponds to C), get the exponent from α, and then compute the value\n",
    "    γ = coordinate[2] |> i-> β[i] |> e-> 2.0^e; # Nice! grab the col (corresponds to γ), get the exponent from β, and then compute the value\n",
    "\n",
    "    C,γ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db671b-1b5a-4afb-990e-3ce8bd784158",
   "metadata": {},
   "source": [
    "Finally, estimate the `best_model::LIBSVM.SVM`, the best predicted label vector `ŷ_test_best::Array{Int64,1}`, and the actual label vector `y_test::Array{Int64,1}` using the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a16e8e86-49ee-41e9-b935-f8fce18f882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, ŷ_test_best, y_test = let\n",
    "\n",
    "    # Training data setup -\n",
    "    D₁ = training; # what dataset are we looking at?\n",
    "    number_of_training_examples = size(D₁,1); # how many rows?\n",
    "    X₁ = [D₁[:,1:end-1] ones(number_of_training_examples)] |> transpose |> Matrix; # augmented features (arranged as m x n)\n",
    "    y₁ = D₁[:,end]; # label\n",
    "\n",
    "    # Test data setup -\n",
    "    D₂ = test; # what dataset are we looking at?\n",
    "    number_of_test_examples = size(D₂,1); # how many rows?\n",
    "    X₂ = [D₂[:,1:end-1] ones(number_of_test_examples)] |> transpose |> Matrix; # features (arranged as m x n)\n",
    "    y₂ = D₂[:,end]; # label\n",
    "\n",
    "    # estimate the best model -\n",
    "    best_model = svmtrain(X₁, y₁, kernel=LIBSVM.Kernel.RadialBasis, \n",
    "                    verbose = false, nu = C_best, gamma = γ_best, svmtype = LIBSVM.NuSVC)\n",
    "    \n",
    "\n",
    "    # compute the ŷ_best -\n",
    "    ŷ_best, _ = svmpredict(best_model,X₂);\n",
    "\n",
    "    # return -\n",
    "    best_model, ŷ_best, y₂\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da36dd-5158-4179-92db-22685d733c49",
   "metadata": {},
   "source": [
    "Confirm the accuracy of the `best_model::LIBSVM.`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81cb4bb2-606f-466f-a8ca-bc03112e9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_best_confirm = confusion(y_test, ŷ_test_best) |> CM -> CM[1,1] + CM[2,2] |> correct -> correct/size(test,1) # impressive!\n",
    "@assert best_accuracy == accuracy_best_confirm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
